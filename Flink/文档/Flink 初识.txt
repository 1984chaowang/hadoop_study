checkpoint
kafka-flink-kafka 
readFile
===TODO===
checkpoint检查点HDFS未发布测试
flink多数据源 

从文件中读数据
Window:
数据流是无限的,无界限. 但是可以通过一个有界的范围来处理无界的数据流.
滚动窗口:每个界限计算结果互不影响(不重合) 比如几分钟统一一次
滑动窗口:每个计算窗口统计数据是有重复的  每30s统计过去一分钟的过车数量

流处理如何解释时间??
Time

Timer
定时器,作为window的触发源,分为两类:
WallTime Timer:按照正常的现实时间作为触发源
LowWatermark Timer:以低水位作为触发源 

low watermark :最低水位
其实就是一个时间戳 ,每一个计算节点都会维护一个时间戳作为watermark
A的低水位值不只和A本身的最旧数据有关,也跟上游的低水位有关.
因此,只要上游还有更旧的数据存在,就会通过低水位机制维护的low watermark告知下游,
下游便会更新它自己的low watermark并且由于lwm timer未触发，因此会进行等待

在一定程度上保证数据的完整性和实效性,但是如果有数据比lowwatermark还晚到达仍没有办法解决
比如:数据在没有进入流系统之前就耽搁了,那low watermark根本不知道
flink为了解决这个问题,还有allow lateness参数,即Window被low watermark timer触发后,
还会等待allow lateness时间才开始计算,但这样会损失一定的实时性

join:双流转换成单流
coGroup+innerjoin



状态status和检查点checkpoint

status:


checkpoint:程序指定时间定期生成 ,保留当前时间的算子的状态


savepoint:用户通过命令行触发,
	存储格式跟checkpoint不相同的 ,savepoint会按照一个标准的格式存储,不管配置什么样
	flink都会从这个checkpoint恢复,常用于版本升级
External Checkpoint:(外部checkpoint)做完一次checkpoint后在制定的目录中 
	多存储一份checkpoint 保留meta数据 双备份
	作业失败或取消状态结束时,外部存储的meta数据将保留下来

##State 和 checkpoint数据的存储方式:
MemoryStateBackend
FsStateBackend
RockDBStateBackend
数据量小存储在 MemoryStateBackend 和 FsStateBackend中
数据量大  存储在RockDBStateBackend 中

------------------
Flink是通过检查点的方式来实现 exactly-once 只执行一次,当遇到故障时将系统重置为初始状态


---------------------------------------------------

## Flink Blob:
JobManager-blob服务 ,也是在flink-conf.yaml文件中配置 
	接收jar包/发送jar包到Taskmanager,传输log文件
## Flink 通信模型(via Akka)
Flink客户端(JobClient)  JobManager(1)  TaskManager(N) 之间通信都是基于Akka actor模型

JobClient 从用户处获取到Flink job,提交给JobManager
	1:JobManager 负责这个job的执行:首先分配所需的(slot:CPU,内存)资源,即TaskManagers上要执行的slot
	2:获取到slot后,jobmanager 部署单独的任务到响应的TaskManager上
	TaskManager产生一个线程来执行这个任务 
	3:状态改变时(开始计算,结束计算,每一次算子计算),状态会被发送回JobManager
	  基于这些状态的更新,JobManager将引导着个job执行完成
	4:一旦执行完,结果将会发送回JobClient

## JobManager和TaskManager
JobManager是核心控制单元,负责整个Flink Job,负责资源分配,任务调度和状态汇报
//TODO



flink  反压机制(backpressure)
产生原因:短时负载高峰导致系统接收数据的速率远高于它处理数据的速率
flink利用自身作为纯数据流引擎的优势来优雅地响应反压问题
Flink 是如何在 Task 之间传输数据的，以及数据流如何实现自然降速?
运行时: 
	operators组件  
		每个operator会消费中间态的流,并在流上进行转换,然后生成新的流
	streams组件

Flink中的反压:
	Flink 使用了高效有界的分布式阻塞队列,就像Java通用的阻塞队列(BlockingQueue)
		Java使用BlockingQueue时:一个较慢的接受者会降低发送者的发送速率,因为一旦队列
		满了(有界队列)发送者会被阻塞
在 Flink 中,这些分布式阻塞队列就是这些逻辑流,而队列容量是通过缓冲池来(LocalBufferPool)实现的
每个生产和被消费的流都会被分配一个缓冲池.
缓冲池管理着一组缓冲(Buffer),缓冲在被消费后可以被回收循环利用.

网络传输中的内存管理:

###################### Flink程序发布  ###################################

程序发布出问题: 
	1:程序发布找不到容器地址 Failed to retrieve JobManager address  2: 找不到日志路径 程序发布需要先创建log目录
1原因是在flink创建cluster后的的一定范围
nohup bin/yarn-session.sh \
--container 5 \
--jobManagerMemory 4024 \
--taskManagerMemory 4048 \
--slots 10 \
--jar $FLINK_HOME/lib/flink-connector-kafka-0.10_2.11-1.3.0.jar,$FLINK_HOME/lib/flink-dist_2.11-1.3.0.jar,$FLINK_HOME/lib/kafka-clients-0.10.0.0.jar,$FLINK_HOME/lib/jedis-2.9.0.jar,$FLINK_HOME/lib/commons-pool2-2.4.2.jar \
>/opt/MtdapProgram/mtdap3/flink-cluster/log/yarn-session.log 2>&1 &

二次识别套牌车
nohup bin/flink run \
--class com.enjoyor.mtdap3.realtime.SrFakeVehicle \
/opt/MtdapProgram/mtdap3/mtdap3-rtc-sr-vehicletrack-1.0/mtdap3-rtc-srfakevehicle-1.0.jar >/opt/MtdapProgram/mtdap3/mtdap3-rtc-sr-vehicletrack-1.0/log/mtdap3-rtc-srfakevehicle-1.0 2>&1 &

flink发布命令并没有指定yarn 怎么就在yarn上运行了?
配置一个容器,再发布一个flink程序  会自动找到 JobManager address (如果创建容器的时间和程序发布的时间间隔太久 会抛找不到jobManager address的异常)

=================AM RM==========================
Flink log中的 RM AM 指的是yarn上的一个ResourceManager 和若干个ApplicationMaster  指的是yarn的AM RM 通信
ApplicationMaster管理在yarn上运行的应用程序的每个实例
	同时负责协调来自 ResourceManager 的资源,并通过NodeManager监视容器的执行和资源使用（CPU、内存等的资源分配）

-------------
雅虎15年测试:
Storm 能够承受每秒 40 万事件,但受限于 CPU； 
Flink 则可以达到每秒 300万事件(7.5 倍)但受限于 Kafka 集群和 Flink 集群之间的网络

Flink 的执行过程是基于流的，这意味着各个处理阶段有更多的重叠,并且混洗操作是流水线式的,因此磁盘访问操作
更少.相反, MapReduce、Tez和Spark是基于批的,这意味着数据在通过
网络传输之前必须先被写入磁盘.该测试说明,在使用 Flink 时,系统空闲时间和磁盘访问操作更少。

接收器  数据源 

Kafka position也是由Flink自己维护的

理想下  无边际数据流 源源不断来  按照时间窗口 计算  输出

现实情况是: 数据不是按时来的 有延迟

所以划分为事件时间  摄取时间  处理时间

公司测试环境下的flink程序,运行26d了也没出现问题,原因在于flink使用的是自己的内存管理体系


===flink内存管理===
主流实时计算框架都是基于jvm语言开发的(Java Scala)
为了加快计算,通常都是将数据加载在内存中,由于数据量巨大,对内存造成很大压力
==数据存储==
最简单做法试封装成对象直接存储在List或Map这样的数据结构中(
	公司从mq中拿到的实时计算生产到的数据通过消费者程序写入到hbase 
	kafka  json  map  list(map) hbase)
引发两个问题?
1:数据规模大时,需要创建的对象非常多(数据加上存储的数据结构,耗费大量内存)
	可能引发OOM
2:源源不断的数据需要被处理,对象持续产生并需要被销毁
	GC压力大
SO:
JVM自带的GC无法满足高效+稳定的流处理,Flink建立一套自己的内存管理体系

Flink将内存分为3个部分(network buffers,Memory Manager pool,Remaining Heap)
每个部分都有不同用途;
1:Network buffers:一些以32KB Byte数组为单位的buffer,主要被网络模块用于数据的网络传输。
	在Flink中主要是基于Netty进行网络传输
2:Memory Manager pool大量以32KB Byte数组为单位的内存池,所有的运行时算法(例如Sort/Shuffle/Join)都从这个内存池申请内存，
       并将序列化后的数据存储其中，结束后释放回内存池
   内存池,由多个MemorySegment组成,每个MemorySegment代表一块连续的内存空间 byte[]数据结构存储  默认32kb
   
3:Remaining(Free)Heap主要留给UDF中用户自己创建的Java对象,由JVM管理.
	用在UDF中用户自己创建的对象 在UDF中,用户流式的处理数据 并不需要太大内存
	同时flink也不建议在UDF中缓存很多数据


重点:
Flink的主动内存管理避免了令人讨厌的OutOfMemoryErrors杀死JVM并减少垃圾收集开销的问题。
Flink具有高效的数据去/序列化堆栈，有助于对二进制数据进行操作，并使更多数据适合内存。
Flink的DBMS风格的运算符本身在二进制数据上运行，在必要时可以在内存中高性能地转移到磁盘。

===Lambda架构===
Lambda 架构用定期运行的批处理作业来实现应用程序的持续性，并通过流
处理器获得预警。流处理器实时提供近似结果；批处理层最终会对近似结果予以纠正

批处理架构很难解决乱序事件流问题
批处理作业的界限不清晰,写死了 假设需要根据产生数据的时间段(如从用户登录到退出)生成
聚合结果，而不是简单地以小时为单位分割数据


=flink流表对偶性=
流和动态表(Dynamic Table)的对偶(duality)性。


======流式SQL=====
SQL 声明式语言

批处理实例:
SELECT a.id FROM A a,B b WHERE a.id=b.id;	
	最简单的双流join,找出表A和表B中相同的id
两个经典的Join算法(归并连接 && 哈希连接):
归并连接算法:拿到两表后,将id由小到大排好序,然后从前往后同时遍历两表
				一旦遇到相等的id就输出一条Join结果(1,排序2,合并及连接)
哈希连接算法:拿到两张表,对数据规模就行一个评估,然后选取较小一张表
			 ,以id作为key,以数据行作为value,建立哈希索引.
			 接着便利另一张大表,对每一个id值到建立好的哈希索引中查找有没有id相等的数据行,
			 有则Join输出
区别:哈希连接算法中,只需要将较小的一张表加载到内存
	  归并连接算法,需要将两张表都加在到内存中
	  
	  
流式场景下的持续查询:
持续查询:没有外界干预的情况下,查询会一直输出结果,不会停止
由于数据流本身无穷,所以在上面的查询都是持续查询,每当有新数据的到来,可能都会有新的查询结果产生
	A B两张表双流Join ,表中数据不断增加,当A B存在相同id,才会生成相应的Join结果
问题:传统数据库允许对一张表进行全表扫描,但是流式场景却做不到
原因:1:我们无法确定下一条数据属于表A还是表B 2:流数据具有无尽性,找不到表的边界
所以传统的归并连接跟哈希连接算法不适用于流式SQL

-------------------Flink 流式SQL 提供多种Join

 


==========================
堆外内存(off-heap),堆内存(on-heap)
https://blog.csdn.net/u010722938/article/details/51558315

flink on yarn
Client提交App到RM上面去运行，然后RM分配第一个container去运行AM，然后由AM去负责资源的监督和管理。
需要说明的是，Flink的yarn模式更加类似spark on yarn的cluster模式，在cluster模式中，dirver将作为AM中的一个线程去运行，
在Flink on yarn模式也是会将JobManager启动在container里面，去做个driver类似的task调度和分配，
YARN AM与Flink JobManager在同一个Container中，这样AM可以知道Flink JobManager的地址，
从而AM可以申请Container去启动Flink TaskManager。待Flink成功运行在YARN集群上，
Flink YARN Client就可以提交Flink Job到Flink JobManager，并进行后续的映射、调度和计算处理。


批流是怎样统一的?
Batch和streaming会有两个不同的ExecutionEnvironment,不同的ExecutionEnvironment会将不同的API翻译成不同的JobGgrah,
JobGraph 之上除了 StreamGraph 还有 OptimizedPlan.OptimizedPlan 是由 Batch API 转换而来的.
StreamGraph 是由 Stream API 转换而来的,JobGraph 的责任就是统一 Batch 和 Stream 的图.

Flink JobManagerHA ?
与Storm不同的是，知道Storm在遇到异常的时候是非常简单粗暴的，
比如说有发生了异常，可能用户没有在代码中进行比较规范的异常处(至少执行一次)的语义，
比如说一个网络超时的异常对他而言影响可能并没有那么大，
但是Flink不同的是他对异常的容忍度是非常的苛刻的，那时候就考虑的是比如说会发生节点或者是网络的故障，
那JobManager单点问题可能就是一个瓶颈，JobManager那个如果挂掉的话，
那么可能对整个作业的影响就是不可恢复的，所以考虑了做HA



监控报警


==flink学习==
flink人大视频 
https://www.bilibili.com/video/av42427050/




0.27896551724137930164268727705113
0.02821177996580390307175284476151

9.8882636111411375865446581108976

===Blink===
2019.2开源的Blink版本,主要是基于flink 1.5.1
看看就行 还是用Apache flink 

==========钉钉============
1.先对flink有宏观视野上理解，看阿里大会整理的文档资料，其中也有可运行的demo。
2.看各个flink 大V的博客，对架构和原理逐步深入理解（可以微信和google搜索关键词）
3.英文好，最好是看Apache flink官网文档，既全面又权威(先看前面后看官网，国人帮整理，理解速度会快些)
4.基于以上3方面理解基础上，自己有相当的技术基础及理解能力 + 毅力恒心 + 热爱程度，可以去啃下源代码  基本问题不大(往下又分2种情况 1.有基础曾经研究并扩展过开源代码  2.无基础 无经验  花费时间就要长些，需要毅力  有人指点会快些)
 ###从现象到本质再到解决方案  比如这个:追踪flink上传的jar放到那个目录 : http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/
===告警===

Apache Committer 2017.7月 ali 5个   
参考:http://wuchong.me/blog/2019/02/12/how-to-become-apache-committer/ 


=================flink开发步骤==========
第一步:构建环境
val streamEnv = StreamExecutionEnvironment.getExecutionEnvironment
streamEnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
第二步:添加数据源
val prop:Properties = new Properties()
prop.setProperty("bootstrap.servers",kafkaProp.getProperty("bootstrap.servers"))
val consumer010= new FlinkKafkaConsumer010(kafkaProp.getProperty("source.topic")
				 ,new SimpleStringSchema(),prop)
consumer010.setStartFromLatest()
val dataStream = streamEnv.addSource(consumer010)
第三步:数据预处理
val outputStream = dataStream
.map(x=>getRecord(x))
.filter(!_._1.isEmpty)
.map(x=>recordProcess(x)) 
第四步:设置时间戳和水印
.assignTimestampsAndWatermarks(new TimestampExtractor(basicProp.getProperty("job.interval").toInt))
第五步:数据分组
.keyBy(0)
第六步:指定时间窗口+聚合计算+输出格式
.timeWindow(Time.seconds(basicProp.getProperty("max.lagged.time").toInt))
.reduce((v1,v2)=>(v1._1,v1._2,v1._3+v2._3,v1._4+v2._4))
.map(x=>toJson(x))
第七步:输出
outputStream.addSink(producer010)
第八步:执行flink
env.execute(basicProp.getProperty("application.name"))


=======================transformation========================
flink 常用转换算子:
map,flatMap,filter,keyBy,reduce,fold,aggregations,window,WindowAll,
Union,Window join,Split,Select,Project
dataSource.map(getRecord(_))
  .filter(new FilterFunction[(String, String, String, Long)] {
	  override def filter(t: (String, String, String, Long)): Boolean = {
		  t._4 match {
			  case t._4 if (t._4) > 20 => true
			  case t._4 if (t._4) <= 20 => false
		  }
	  }
  })
  /*上下等同,相当于是源码中类似这样已经实现*/
  .filter(_._4>20)
 
#### 调用filter算子()可以通过重写FilterFunction接口来实现 filter方法 
  
==================================Flink 双流转换================================
算子:coGroup join coflatmap

Join:只输出匹配成功的数据
CoGroup:无论是否匹配都会输出
CoFlatMap:没有匹配操作,只是分别接收两个流的输入

-------------------------
join,coGroup实现代码结构
val stream1 = ...
val stream2 = ...

stream1.join(stream2)
    .where(_._1).equalTo(_._1) //join的条件stream1中的某个字段和stream2中的字段值相等
    .window(...) // 指定window，stream1和stream2中的数据会进入到该window中。只有该window中的数据才会被后续操作join
    .apply((t1, t2, out: Collector[String]) => {
      out.collect(...) // 捕获到匹配的数据t1和t2，在这里可以进行组装等操作
    })
    .print()
--------------------------------

==========背压============
参照:https://yq.aliyun.com/articles/64821

===parallelism  && slot====
parallelism 并行度 默认1 
程序中设置 env.setParallelism(3); 这里设置的是全局的,包含下面执行的每一个算子
为每一个算子单独设置并行度: dataStream.map(new XxxMapFunction).setParallelism(5)  
优先级:算子设置并行度 > env 设置并行度 > 配置文件默认并行度	
	eg:如果在代码中设置的是env.setParallelism(5),flink-conf.yaml文件中默认为1(1.4) 发布上flink-cluster 上会使用5个slot 


slot:作用,是指taskmanager的并发执行能力,简单理解是将CPU和内存分成一个个逻辑单位,即slot


================Flink算子====================

----------------------------数据流分组-----------------------------
keyBy 用于指定数据流是否进行分组
	需要在window函数前指定好,使用keyBy(...)可以将数据流拆分成逻辑分组的数据流
	如果不使用keyBy,你的数据流不是分组的
	分组数据流将你的window计算通过多任务并发执行,每一个逻辑分组流在执行中与其他的逻辑分组流是独立地进行的
在非分组的数据流中,你的原始数据并不会拆分成多个逻辑流并且所有的window逻辑都在一个任务中执行,并发度为1





